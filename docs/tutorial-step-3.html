<!DOCTYPE html>
<html>
  <body>
    <button id="play">⏯️</button>
    <button id="stop">⏹</button>

    <script src="https://unpkg.com/helicon@0.1.0/dist/index.js"></script>
    <script id="sequenceProcessor" type="text/template">
      // Define a processor node for our sequence.  `registorProcessor` is a
      // global function in the audio worklet scope.  We give it a name and pass
      // a subclass of AudioWorkletProcessor.
      registerProcessor(
        'SequenceProcessor',
        class SequenceProcessor extends AudioWorkletProcessor {
          // This tells the audio API which parameters to make available on
          // worklet nodes using this processor
          static get parameterDescriptors() {
            return [
              {
                name: 'clock',
                defaultValue: 0,
                // there are two possible values for automationRate - a-rate
                // meaning the sample rate of the parameter input will match the
                // sample rate of audio output.  k-rate means that we will have
                // a single value of the parameter per buffer.  A typical sample
                // rate is 48000hz, buffer size is 128, and we need two samples
                // per cycle, so using k-rate lets us clock this up to 187hz.
                automationRate: 'k-rate',
              },
            ];
          }

          // the sequence processor needs to keep track of its position and the
          // previous value of clock between processing frames.
          lastClock = 0;
          position = 0;

          constructor(options) {
            super(options);
            this.sequence = options.processorOptions.sequence;
            // we don't update the graph in this tutorial, communication between
            // the main and audio threads is only possible through message
            // passing - so this is used to pass updates to the sequence.
            this.port.onmessage = ({ data }) => {
              this.sequence = data;
              // when the sequence is udpated, we make sure the current position
              // is not larger than the length of the sequence.
              this.position =
                data.length === 0 ? 0 : this.position % data.length;
            };
          }

          // The most important part of the processor itself is a process
          // method, which recieves buffers for its inputs and parameters and
          // fills buffers for its output.
          process(
            // inputs and outputs here are arrays of inputs, each of which is
            // an array of channels, with each channel being a Float32Array whos
            // values represent individual samples
            inputs,
            [[output]],
            // Parameters are passed as an object, with parameter names as keys
            // and Float32Arrays as values.  For k-rate parameters like clock,
            // the array only has a single element.
            { clock: [clock] },
          ) {
            // if the value of the clock param rises across zero, we increment
            // the position
            if (this.lastClock < 0 && clock >= 0) {
              this.position = (this.position + 1) % this.sequence.length;
            }
            this.lastClock = clock;

            // we fill the output with the value from the sequence at the
            // current position
            output.fill(this.sequence[this.position]);

            // returning true from process tells the web audio API to keep the
            // node alive
            return true;
          }
        },
      );
    </script>
    <script>
      const { AudioGraph, ParamType, build } = helicon;

      // create extension referencing our processor
      const extensions = [
        {
          // `type` here names the new node we are adding - we will use this in
          // `build` below
          type: 'SequenceNode',
          // this description is used internally by helicon to construct nodes.
          // params here describes both their true audio parameters (like clock
          // here) and any other values that can be passed from the virtual
          // graph to the worklet processor (like sequence here)
          description: {
            numberOfInputs: 0,
            numberOfOutputs: 1,
            params: {
              clock: { type: ParamType.AudioParam, default: 0 },
              sequence: {
                type: ParamType.Value,
                default: [],
              },
            },
          },
          // This is the main thread AudioNode class - its constructor
          // configures the number of inputs and outputs and passes data to the
          // worklet processor via processorOptions and parameterValues.
          node: class SequenceNode extends AudioWorkletNode {
            constructor(context, { clock, sequence = [] }) {
              super(context, 'SequenceProcessor', {
                numberOfInputs: 0,
                numberOfOutputs: 1,
                outputChannelCount: [1],
                channelCount: 1,
                channelCountMode: 'explicit',
                channelInterpretation: 'discrete',
                processorOptions: { sequence },
                parameterValues: { clock },
              });
              this.clock = this.parameters.get('clock');
              this._seqeuence = sequence;
            }

            // This is the other side of the message passing code we saw in the
            // processor.  When sequence changes on the main thread, we need to
            // pass it to the audio thread via postMessage.  We use
            // getters/setters here to make sure that changes are not missed.
            // This is a common pattern for main/audio thread communication that
            // we will use any time we need to send data outside of audio
            // params.
            get sequence() {
              return this._sequence;
            }
            set sequence(sequence) {
              this._sequence = sequence;
              this.port.postMessage(new Float32Array(sequence));
            }
          },
          processor: `data:application/javascript;base64,${btoa(
            document.getElementById('sequenceProcessor').innerText,
          )}`,
        },
      ];

      // create graph playing a 220hz tone through a lowpass filter
      const ag = new AudioGraph(
        build((node, edge) => {
          node('CLOCK', 'OscillatorNode', { type: 'square', frequency: 4 });
          node('SEQ', 'SequenceNode', {
            // The values here are in cents, or 100ths of semitone.  We will
            // connect the output of our sequence to the detune parameter of our
            // oscillator node.  This sequence is a major 7th arepggio.
            sequence: [0, 400, 700, 1100, 1200, 1100, 700, 400],
          });
          node('OSC', 'OscillatorNode', { type: 'sawtooth', frequency: 220 });
          node('FILT', 'BiquadFilterNode', {
            frequency: 440,
            Q: 4,
          });
          node('DEST', 'AudioDestinationNode');

          edge('CLOCK', 0, 'SEQ', 'clock');
          edge('SEQ', 0, 'OSC', 'detune');
          edge('OSC', 0, 'FILT', 0);
          edge('FILT', 0, 'DEST', 0);
        }, extensions),
        extensions,
      );

      // make buttons control playback
      document
        .getElementById('play')
        .addEventListener('click', () => (ag.playing ? ag.pause() : ag.play()));
      document
        .getElementById('stop')
        .addEventListener('click', () => ag.stop());
    </script>
  </body>
</html>
